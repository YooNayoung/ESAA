{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0509session.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "11QTjee86YGwlFl4cNhD7yENI77UxhA7-",
      "authorship_tag": "ABX9TyNkBmdNE4y6gSrJqWIHmPr5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YooNayoung/ESAA/blob/main/0509session.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[NLP 언제까지 미룰래? 일단 들어와!!] #4. word embedding**"
      ],
      "metadata": {
        "id": "6VQQSyIp_Ti9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Embedding이란?**\n",
        "- vectorization의 치명적인 단점은 바로 단어나 문장들 사이의 관계에 대해서 설명하지 못한다는 것.\n",
        "- 이 때 사용되는 것이 Embedding 기법이며 word2vec을 비롯한 다양한 임베딩 기법들이 존재함.\n",
        "- 비슷한 의미를 내포하고 있는 토큰들은 서로 가깝게, 그렇지 않은 토큰들은 서로 멀리 뿌리도록 하는 것이 임베딩의 목적. 검색 시스템, 감성 분석 등에서는 훌륭한 임베딩을 수행하는 것이 전체 문제 해결에 많은 영향을 줌.\n",
        "- 임베딩 또한 하나의 모델을 의미하며 훈련이 필요. 데이터가 충분하고 시간이 많으면 소지한 데이터에 특화된 임베딩 모델을 학습시킬 수 있음. 보통은 pre_trained embedding model을 가져와서 사용함."
      ],
      "metadata": {
        "id": "EcEt-fxj_aJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.Keras Embedding Layer**\n",
        "- 기본적으로 가장 쉽고 빠르게 네트워크 모델에 임베딩 층을 주입할 수 있는 방식. \n",
        "- 무작위로 특정 차원으로 입력 벡터들을 뿌린 후 학습을 통해 가중치들을 조정해 나감. 즉, 단어 사이의 관계를 반영하는 방법이 아님."
      ],
      "metadata": {
        "id": "xmc3yQjv_870"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train = pd.read_csv('/content/drive/MyDrive/ESAA(OB)/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/ESAA(OB)/test_x.csv')\n",
        "submission = pd.read_csv('/content/drive/MyDrive/ESAA(OB)/sample_submission.csv', index_col=False)"
      ],
      "metadata": {
        "id": "5Wux1GwMMKPk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def text2sequence(train_text, max_len=100):\n",
        "    \n",
        "    tokenizer = Tokenizer() #keras의 vectorizing 함수 호출\n",
        "    tokenizer.fit_on_texts(train_text) #train 문장에 fit\n",
        "    train_X_seq = tokenizer.texts_to_sequences(train_text) #각 토큰들에 정수 부여\n",
        "    vocab_size = len(tokenizer.word_index) + 1 #모델에 알려줄 vocabulary의 크기 계산\n",
        "    print('vocab_size : ', vocab_size)\n",
        "    X_train = pad_sequences(train_X_seq, maxlen = max_len) #설정한 문장의 최대 길이만큼 padding\n",
        "    \n",
        "    return X_train, vocab_size, tokenizer\n",
        "\n",
        "train_X, vocab_size, tokenizer = text2sequence(train['text'], max_len = 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEuDifehL-aW",
        "outputId": "5696069c-23c8-44cf-9346-604f5cf72c52"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size :  42331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding"
      ],
      "metadata": {
        "id": "uQ_Qau09JSDP"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 128, input_length = 100))"
      ],
      "metadata": {
        "id": "aXOlggeSABMg"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. word2vec**\n",
        "- word2vec의 핵심 아이디어는 \"친구를 보면 그 사람을 알 수 있다\"입니다. 주변 단어와의 관계를 통해 해당 단어의 의미적 특성을 파악함.\n",
        "- word2vec embedding matrix를 kears의 embedding에 주입하는 과정은 다음과 같음."
      ],
      "metadata": {
        "id": "NPI3dmyOMpLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 구글의 사전 훈련된 word2vec bin 파일을 다운로드"
      ],
      "metadata": {
        "id": "9FzTuaLlM0V3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. gensim 모듈과 bin파일을 활용해 word2vec 모델을 로드"
      ],
      "metadata": {
        "id": "wuP_pbuSNZHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim  \n",
        "word2vec = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/ESAA(OB)/GoogleNews-vectors-negative300.bin.gz', binary = True)"
      ],
      "metadata": {
        "id": "IrNa0OKAM3-G"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. vocabulary에 있는 토큰들의 벡터를 가져와 embedding matrix에 저장"
      ],
      "metadata": {
        "id": "PETJLVQJNgzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 300)) #300차원의 임베딩 매트릭스 생성\n",
        "\n",
        "for index, word in enumerate(tokenizer.word_index): #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.\n",
        "    if word in word2vec: #넘겨 받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 뜻)\n",
        "        embedding_vector = word2vec[word] #해당 토큰에 해당하는 vector를 불러오고\n",
        "        embedding_matrix[index] = embedding_vector #해당 위치의 embedding_mxtrix에 저장합니다.\n",
        "    else:\n",
        "        print(\"word2vec에 없는 단어입니다.\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDCsqtQ_NYD_",
        "outputId": "f7409fad-937b-4280-97c3-b9bcb1211446"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word2vec에 없는 단어입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. keras embedding layer에 embedding_matrix를 가중치로 주어 이용합니다."
      ],
      "metadata": {
        "id": "CPufQ0voNj_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 300,weights = [embedding_matrix], input_length = 100))"
      ],
      "metadata": {
        "id": "7GIUdi7RNYGQ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. glove**\n",
        "- glove는 word2vec의 단점을 보완하기 위하여 등장함. word2vec이 사용자가 지정한 주변 단어의 개수에 대해서만 학습이 이루어지기 때문에 데이터 전체에 대한 정보를 담기 어렵다는 단점을 지적하였음.\n",
        "- glove의 핵심 아이디어는 \"각 토큰들 간의 유사성은 그대로 가져가면서 데이터 전체에 대한 빈도를 반영하자\"\n",
        "- glove embedding matrix를 keras의 embedding matrix에 주입하는 방법은 다음과 같음."
      ],
      "metadata": {
        "id": "WK2OyYYHNper"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 사전 훈련된 벡터를 갖고 있는 txt 파일 다운로드"
      ],
      "metadata": {
        "id": "taRLXopsODPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. txt 파일에 있는 단어와 벡터들을 glove dictionary에 저장"
      ],
      "metadata": {
        "id": "y-CgRbKyOHun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "2. # load the whole embedding into memory\n",
        "glove = dict()\n",
        "f = open('/content/drive/MyDrive/ESAA(OB)/glove.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vector = np.array(values[1:], dtype='float32')\n",
        "    glove[word] = vector\n",
        "f.close()"
      ],
      "metadata": {
        "id": "F4Ro-4lwOJdt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. vocabulary에 있는 토큰들의 벡터를 가져와 embedding matrix에 저장"
      ],
      "metadata": {
        "id": "kmDJlN8JOMTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 100)) #300차원의 임베딩 매트릭스 생성\n",
        "\n",
        "for index, word in enumerate(tokenizer.word_index): #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.\n",
        "    if word in glove: #넘겨 받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 뜻)\n",
        "        embedding_vector = glove[word] #해당 토큰에 해당하는 vector를 불러오고\n",
        "        embedding_matrix[index] = embedding_vector #해당 위치의 embedding_mxtrix에 저장합니다.\n",
        "    else:\n",
        "        print(\"glove 없는 단어입니다.\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E76nWDHVOOxi",
        "outputId": "d68d9abb-38f3-435d-96a4-f7cb82feca3a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "glove 없는 단어입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. keras embedding layer에 embedding_matrix를 가중치로 주어 이용"
      ],
      "metadata": {
        "id": "hpek-cCCOQNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100,weights = [embedding_matrix], input_length = 100))"
      ],
      "metadata": {
        "id": "jXFFVFMNOS3w"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Fasttext**\n",
        "- Fasttext의 핵심 아이디어는 단어 단위가 아닌 sub 단어를 단위로 사용한다는 것임.\n",
        "- 따라서 미리 학습되지 않은 단어들에 대한 vector도 표현해준다는 장점이 있음.\n"
      ],
      "metadata": {
        "id": "q32MGzLLOUqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 사전 훈련된 bin 파일 다운로드"
      ],
      "metadata": {
        "id": "0mlo6lEcOgAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. vec 파일을 gensim을 활용하여 읽어옴."
      ],
      "metadata": {
        "id": "oHoVZQSlOhyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "FastText = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/ESAA(OB)/fasttext.bin', binary = True, encoding='latin_1')"
      ],
      "metadata": {
        "id": "bErgX6qcOkdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. vocabulary에 있는 토큰들의 벡터를 가져와 embedding matrix에 저장"
      ],
      "metadata": {
        "id": "aXQwBbxqOmLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 300)) #300차원의 임베딩 매트릭스 생성\n",
        "\n",
        "for index, word in enumerate(tokenizer.word_index): #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.\n",
        "    if word in word2vec: #넘겨 받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 뜻)\n",
        "        embedding_vector = word2vec[word] #해당 토큰에 해당하는 vector를 불러오고\n",
        "        embedding_matrix[i] = embedding_vector #해당 위치의 embedding_mxtrix에 저장합니다."
      ],
      "metadata": {
        "id": "lJBZcxiKOwwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. keras embedding layer에 embedding_matrix를 가중치로 주어 이용"
      ],
      "metadata": {
        "id": "a96fB-u0OyHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 300,weights = [embedding_matrix], input_length = 100))"
      ],
      "metadata": {
        "id": "LpZrJzJHOz2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[NLP 언제까지 미룰래? 일단 들어와!!] #5. Modeling(완)**"
      ],
      "metadata": {
        "id": "PPsHt4DUPvai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Modeling**\n",
        "- 현재 ML에서 주로 사용되는 SVM, RF, Boosting Machine등을 활용하여 간단한 NLP 태스크를 해결이 가능하지만 복잡한 문제를 해결하기에는 한계가 있습니다. 따라서 기본적으로 네트워크 모델을 이용하여 NLP 문제를 해결하는 것이 일반적입니다.\n",
        "- NLP 문제를 해결하기 위해서 주로 쓰이는 RNN과 Layer들의 간단한 구조를 소개해보겠습니다."
      ],
      "metadata": {
        "id": "243q5nzpPzmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.RNN**\n",
        "- RNN은 순차적인 구조이며 그 특징 때문에 연산에 많은 시간이 걸리rp 되고 네트워크의 고질적인 문제인 역전파 소실 문제가 발생합니다."
      ],
      "metadata": {
        "id": "Q5N5YILpP9RR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. LSTM**\n",
        "- 이제 LSTM이 RNN 구조에서 특정 시점의 정보를 다음 시점으로 전달할 때 얼만큼의 정보를 전달할지 결정하는 역할을 하겠다는 것을 유추할 수 있습니다.\n",
        "- LSTM의 가장 큰 특징은 기존 RNN에 cell state를 추가한 것입니다. 이 cell state는 입력들의 정보를 선별하여 다음 출력으로 내보내는 게이트 역할을 합니다. 이 과정을 통해 불필요한 정보들을 걸러내어 매끄러운 진행이 가능하고 이로 인해 역전파 소실 문제를 줄여 성능이 증가하게 됩니다. "
      ],
      "metadata": {
        "id": "OTa2F9NEQSJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. GRU**\n",
        "- GRU는 LSTM의 복잡한 구조를 보다 간결하게 보완한 모델입니다. LSTM의 장점을 가져오면서 속도적인 부분을 개선하여 더욱 빠른 속도로 비슷한 성능을 낸다고 알려져 있습니다."
      ],
      "metadata": {
        "id": "dRgopUhSQSNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Attention**\n",
        "- attention 매커니즘은 RNN 모델의 구조적인 한계를 극복한 모델입니다. 입력으로 이용된 정보들을 순차적으로 고정 길이로 압축하면서 발생하는 정보의 손실, 역전파 소실 문제를 해결하기 위해 노력했고 어떤 토큰의 정보가 가장 큰 도움을 줬는지 알 수 있습니다.\n",
        "- 뒤이어 나온 자연어 처리의 세기적 발견인 Transformer(attention is all you need)의 핵심 알고리즘이며 이 둘에 대한 이해가 바탕이 되어야 다양한 문제에서 SOTA를 자랑하는 ELMO, BERT, GPT 등의 방법론들을 이해 할 수 있습니다."
      ],
      "metadata": {
        "id": "AFwxBNaAQVXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. 대회 적용**"
      ],
      "metadata": {
        "id": "3Pqp5oTkQ5UW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 간단한 전처리 + 형태소 분석"
      ],
      "metadata": {
        "id": "-ICtGQZaQ9NU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "apt-get update\n",
        "apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "pip3 install JPype1\n",
        "pip3 install konlpy"
      ],
      "metadata": {
        "id": "MW4qpQhVXtWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env JAVA_HOME \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neePGW6dab2w",
        "outputId": "13b550cd-2b43-47a6-ef60-761d07906d13"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
        "pip3 install /tmp/mecab-python-0.996"
      ],
      "metadata": {
        "id": "iSZ4pEAoaeDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow-addons\n",
        "!pip install -q \"tqdm>=4.36.1\""
      ],
      "metadata": {
        "id": "2dgGwMQabauT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "import tqdm\n",
        "import re\n",
        "def text_preprocessing(text_list):\n",
        "    \n",
        "    stopwords = ['을', '를', '이', '가', '은', '는', 'null']\n",
        "    tokenizer = Okt()\n",
        "    \n",
        "    for text in tqdm.tqdm(text_list):\n",
        "        txt = re.sub('[^가-힣a-z]', ' ', text.lower())\n",
        "        token = tokenizer.morphs(txt)\n",
        "        token = [t for t in token if t not in stopwords or type(t) != float]\n",
        "        \n",
        "    return token, tokenizer\n",
        "\n",
        "train['token'], okt = text_preprocessing(train['text'])"
      ],
      "metadata": {
        "id": "KFDd7uEbQ-Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### vectorization"
      ],
      "metadata": {
        "id": "sEVR7j1-RAqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text2sequence(train_text, max_len=1000):\n",
        "    \n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(train_text)\n",
        "    train_X_seq = tokenizer.texts_to_sequences(train_text)\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    print('vocab_size : ', vocab_size)\n",
        "    X_train = pad_sequences(train_X_seq, maxlen = max_len)\n",
        "    return X_train, vocab_size, tokenizer\n",
        "\n",
        "train_y = train['info']\n",
        "train_X, vocab_size, vectorizer = text2sequence(train['token'], max_len = 100)\n",
        "print(train_X.shape, train_y.shape)"
      ],
      "metadata": {
        "id": "TJeGA6ebRDpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding"
      ],
      "metadata": {
        "id": "1vO-WywdRO4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary = True)\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "\n",
        "for index, word in enumerate(vocabulary):\n",
        "    if word in word2vec\n",
        "        embedding_vector = word2vec[word] \n",
        "        embedding_mxtrix[i] = embedding_vector \n",
        "    else:\n",
        "        print(\"word2vec에 없는 단어입니다.\")\n",
        "        break"
      ],
      "metadata": {
        "id": "XYoPflVFRSZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modeling\n",
        "\n"
      ],
      "metadata": {
        "id": "PQxC9H9mRPAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LSTM(vocab_size, max_len=1000):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, 300,weights = [embedding_matrx], input_length = max_len)) #임베딩 가중치 적용 코드\n",
        "    model.add(SpatialDropout1D(0.3))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation='relu', kernel_regularizer = regularizers.l2(0.001)))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')\n",
        "    model.summary()\n",
        "    return model"
      ],
      "metadata": {
        "id": "TAdcR7ydRXah"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}